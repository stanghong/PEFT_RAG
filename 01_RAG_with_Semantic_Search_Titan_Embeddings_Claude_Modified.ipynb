{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Question & Answering with Amazon Bedrock using LangChain\n",
    "\n",
    "### Challenges\n",
    "\n",
    "When trying to solve a Question Answering task over a larger document corpus with the help of LLMs we need to master the following challenges (amongst others):\n",
    "- How to manage large document(s) that exceed the token limit\n",
    "- How to find the document(s) relevant to the question being asked\n",
    "\n",
    "### Infusing knowledge into LLM-powered systems\n",
    "\n",
    "We have two primary [types of knowledge for LLMs](https://www.pinecone.io/learn/langchain-retrieval-augmentation/): \n",
    "- **Parametric knowledge**: refers to everything the LLM learned during training and acts as a frozen snapshot of the world for the LLM. \n",
    "- **Source knowledge**: covers any information fed into the LLM via the input prompt. \n",
    "\n",
    "When trying to infuse knowledge into a generative AI - powered application we need to choose which of these types to target. Fine-tuning, explored in other workshops, deals with elevating the parametric knowledge through fine-tuning. Since fine-tuning is a resouce intensive operation, this option is well suited for infusing static domain-specific information like domain-specific langauage/writing styles (medical domain, science domain, ...) or optimizing performance towards a very specific task (classification, sentiment analysis, RLHF, instruction-finetuning, ...). \n",
    "\n",
    "In contrast to that, targeting the source knowledge for domain-specific performance uplift is very well suited for all kinds of dynamic information, from knowledge bases in structured and unstructured form up to integration of information from live systems. This Lab is about retrieval-augmented generation, a common design pattern for ingesting domain-specific information through the source knowledge. It is particularily well suited for ingestion of information in form of unstructured text with semi-frequent update cycles. \n",
    "\n",
    "In this notebook we explain how to utilize the RAG (retrieval-agumented generation) pattern originating from [this](https://arxiv.org/pdf/2005.11401.pdf) paper published by Lewis et al in 2021. It is particularily useful for Question Answering by finding and leveraging the most useful excerpts of documents out of a larger document corpus providing answers to the user questions.\n",
    "\n",
    "#### Prepare documents\n",
    "![Embeddings](./images/Embeddings_lang.png)\n",
    "\n",
    "Before being able to answer the questions, the documents must be processed and a stored in a document store index\n",
    "- Load the documents\n",
    "- Process and split them into smaller chunks\n",
    "- Create a numerical vector representation of each chunk using Amazon Bedrock Titan Embeddings model\n",
    "- Create an index using the chunks and the corresponding embeddings\n",
    "#### Ask question\n",
    "![Question](./images/Chatbot_lang.png)\n",
    "\n",
    "When the documents index is prepared, you are ready to ask the questions and relevant documents will be fetched based on the question being asked. Following steps will be executed.\n",
    "- Create an embedding of the input question\n",
    "- Compare the question embedding with the embeddings in the index\n",
    "- Fetch the (top N) relevant document chunks\n",
    "- Add those chunks as part of the context in the prompt\n",
    "- Send the prompt to the model under Amazon Bedrock\n",
    "- Get the contextual answer based on the documents retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usecase\n",
    "#### Dataset\n",
    "In this example, you will use several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "In order to follow the RAG approach this notebook is using the LangChain framework where it has integrations with different services and tools that allow efficient building of patterns such as RAG. We will be using the following tools:\n",
    "\n",
    "- **LLM (Large Language Model)**: Anthropic Claude available through Amazon Bedrock\n",
    "\n",
    "  This model will be used to understand the document chunks and provide an answer in human friendly manner.\n",
    "- **Embeddings Model**: Amazon Titan Embeddings available through Amazon Bedrock\n",
    "\n",
    "  This model will be used to generate a numerical representation of the textual documents\n",
    "\n",
    "- **Document Loader**: \n",
    "    - PDF Loader available through LangChain for PDFs\n",
    "\n",
    "  These are loaders that can load the documents from a source, for the sake of this notebook we are loading the sample files from a local path. This could easily be replaced with a loader to load documents from enterprise internal systems.\n",
    "\n",
    "- **Vector Store**: FAISS available through LangChain\n",
    "  In this notebook we are using this in-memory vector-store to store both the embeddings and the documents. In an enterprise context this could be replaced with a persistent store such as AWS OpenSearch, RDS Postgres with pgVector, ChromaDB, Pinecone or Weaviate.\n",
    "\n",
    "- **Index**: VectorIndex\n",
    "  The index helps to compare the input embedding and the document embeddings to find relevant document.\n",
    "\n",
    "- **Wrapper**: wraps index, vector store, embeddings model and the LLM to abstract away the logic from the user.\n",
    "\n",
    "### Python 3.10\n",
    "\n",
    "⚠️⚠️⚠️ For this lab we need to run the notebook based on a Python 3.10 runtime. ⚠️⚠️⚠️\n",
    "\n",
    "If you carry out the workshop from your local environment outside of the Amazon SageMaker studio please make sure you are running a Python runtime > 3.10.\n",
    "\n",
    "### Setup\n",
    "To run this notebook you would need to install 2 more dependencies, [PyPDF](https://pypi.org/project/pypdf/) and [FAISS vector store](https://github.com/facebookresearch/faiss).\n",
    "\n",
    "Then begin with instantiating the LLM and the Embeddings model. Here we are using Anthropic Claude to demonstrate the use case.\n",
    "\n",
    "Note: It is possible to choose other models available with Bedrock. You can replace the `model_id` as follows to change the model.\n",
    "\n",
    "`llm = Bedrock(model_id=\"...\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --force-reinstall boto3 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "botocore 1.31.85 requires urllib3<1.27,>=1.25.4; python_version < \"3.10\", but you have urllib3 2.1.0 which is incompatible.\n",
      "llama-index 0.8.37 requires urllib3<2, but you have urllib3 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index 0.8.37 requires urllib3<2, but you have urllib3 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain==0.0.305 --force-reinstall --quiet\n",
    "%pip install pypdf==3.8.1 faiss-cpu==1.7.4 --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "botocore 1.31.85 requires urllib3<1.27,>=1.25.4; python_version < \"3.10\", but you have urllib3 2.1.0 which is incompatible.\n",
      "llama-index 0.8.37 requires urllib3<2, but you have urllib3 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken==0.4.0 --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index 0.8.37 requires urllib3<2, but you have urllib3 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sqlalchemy==2.0.21 --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following lines to run from your local environment outside of the AWS account with Bedrock access. If you are carrying the lab out in Amazon SageMaker Studio, you are set without. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['BEDROCK_ASSUME_ROLE'] = '<YOUR_VALUES>'\n",
    "#os.environ['AWS_PROFILE'] = 'bedrock-user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "bedrock_client = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=True # Default. Needed for invoke_model() from the data plane\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.TokenCounterHandler import TokenCounterHandler\n",
    "\n",
    "token_counter = TokenCounterHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup langchain\n",
    "\n",
    "We create an instance of the Bedrock classes for the LLM and the embedding models. At the time of writing, Bedrock supports one embedding model and therefore we do not need to specify any model id. To be able to compare token consumption across the different RAG-approaches shown in the workshop labs we use langchain callbacks to count token consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will be using the Titan Embeddings Model to generate our Embeddings.\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# - create the Anthropic Model\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2\", \n",
    "              client=bedrock_client, \n",
    "              model_kwargs={\n",
    "                  'max_tokens_to_sample': 200\n",
    "              }, \n",
    "              callbacks=[token_counter])\n",
    "\n",
    "# - create the Titan Embeddings Model\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\",\n",
    "                                       client=bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Let's first download some of the files to build our document store.\n",
    "\n",
    "In this example, you will use several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "# urls = [\n",
    "#     'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "#     'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "#     'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "#     'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
    "# ]\n",
    "\n",
    "filenames = [\n",
    "    # \"bedrock.pdf\",\n",
    "\"Data_Scientist_Resume_0.pdf\",\n",
    "\"Data_Scientist_Resume_1.pdf\",\n",
    "\"Data_Scientist_Resume_3.pdf\",\n",
    "\"Data_Scientist_Resume_2.pdf\",\n",
    "\"Data_Scientist_Resume_6.pdf\",\n",
    "# \".DS_Store\",\n",
    "\"Data_Scientist_Resume_7.pdf\",\n",
    "\"Data_Scientist_Resume_5.pdf\",\n",
    "\"Data_Scientist_Resume_4.pdf\",\n",
    "\"Data_Scientist_Resume_10.pdf\",\n",
    "\"Data_Scientist_Resume_9.pdf\",\n",
    "\"Data_Scientist_Resume_8.pdf\",\n",
    "\"Machine_Learning_Engineer_Resume_3.pdf\",\n",
    "\"Machine_Learning_Engineer_Resume_2.pdf\",\n",
    "\"Machine_Learning_Engineer_Resume_5.pdf\",\n",
    "\"Machine_Learning_Engineer_Resume_4.pdf\",\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(id=0, source=filenames[0]),\n",
    "    dict(id=1, source=filenames[1]),\n",
    "    dict(id=2, source=filenames[2]),\n",
    "    dict(id=3, source=filenames[3]),\n",
    "    dict(id=4, source=filenames[4]),\n",
    "    dict(id=5, source=filenames[5]),\n",
    "    dict(id=6, source=filenames[6]),\n",
    "    dict(id=7, source=filenames[7]),\n",
    "    dict(id=8, source=filenames[8]),\n",
    "    dict(id=9, source=filenames[9]),\n",
    "    dict(id=10, source=filenames[10]),\n",
    "    dict(id=11, source=filenames[11]),\n",
    "    dict(id=12, source=filenames[12]),\n",
    "    dict(id=13, source=filenames[13]),\n",
    "    dict(id=14, source=filenames[14]),\n",
    "    # dict(year=2020, source=filenames[2]),\n",
    "    # dict(year=2019, source=filenames[3])\n",
    "    ]\n",
    "\n",
    "data_root = \"./data/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of Amazon's culture, the CEO always includes a copy of the 1997 Letter to Shareholders with every new release. This will cause repetition, take longer to generate embeddings, and may skew your results. In the next section you will take the downloaded data, trim the 1997 letter (last 3 pages) and overwrite them as processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pypdf import PdfReader, PdfWriter\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading we can load the documents with the help of [DirectoryLoader from PyPDF available under LangChain](https://python.langchain.com/en/latest/reference/modules/document_loaders.html) and splitting them into smaller chunks.\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. Also the embeddings model has a limit of the length of input tokens limited to 512 tokens, which roughly translates to ~2000 characters. For the sake of this use-case we are creating chunks of roughly 1000 characters with an overlap of 100 characters using [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [Document(page_content='Data Scientist Resume\\nContact Information\\nName: Alex Johnson\\nAddress: 123 Main St, Anytown, USA\\nPhone: (555) 123-4567\\nEmail: alex.johnson@example.com\\nLinkedIn: linkedin.com/in/alexjohnson\\nGitHub: github.com/alexjohnson\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, State University, 2019\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, Tech Solutions Inc., June 2019 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 0, 'source': 'Data_Scientist_Resume_0.pdf'}), Document(page_content='Data Visualization: Matplotlib, Seaborn, Tableau\\nMachine Learning: Scikit-learn, TensorFlow, Keras\\nBig Data: Hadoop, Spark\\nCertifications\\nCertified Data Scientist, Data Science Institute\\nAdvanced SQL, Coursera\\nProjects\\nProject A: Developed a recommendation system to personalize content for users, which increased user engagement by\\n20%.\\nProject B: Implemented a fraud detection model for financial transactions that decreased fraudulent activity by 35%.', metadata={'id': 0, 'source': 'Data_Scientist_Resume_0.pdf'})]\n",
      "\n",
      "2 [Document(page_content='Data Scientist Resume\\nContact Information\\nName: Alex Johnson\\nAddress: 123 Main St, Anytown, USA\\nPhone: (555) 123-4567\\nEmail: alex.johnson@example.com\\nLinkedIn: linkedin.com/in/alexjohnson\\nGitHub: github.com/alexjohnson\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, State University, 2019\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, Tech Solutions Inc., June 2019 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 1, 'source': 'Data_Scientist_Resume_1.pdf'}), Document(page_content='Data Visualization: Matplotlib, Seaborn, Tableau\\nMachine Learning: Scikit-learn, TensorFlow, Keras\\nBig Data: Hadoop, Spark\\nCertifications\\nCertified Data Scientist, Data Science Institute\\nAdvanced SQL, Coursera\\nProjects\\nProject A: Developed a recommendation system to personalize content for users, which increased user engagement by\\n20%.\\nProject B: Implemented a fraud detection model for financial transactions that decreased fraudulent activity by 35%.', metadata={'id': 1, 'source': 'Data_Scientist_Resume_1.pdf'})]\n",
      "\n",
      "2 [Document(page_content='Data Scientist Resume\\nContact Information\\nName: Michael Smith\\nAddress: 789 Pine Road, Datasphere, USA\\nPhone: (555) 998-1234\\nEmail: michael.smith@example.com\\nLinkedIn: linkedin.com/in/michaelsmith\\nGitHub: github.com/michaelsmith\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, International Tech College, 2020\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, DataMax Corp., August 2020 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 2, 'source': 'Data_Scientist_Resume_3.pdf'}), Document(page_content='Data Visualization: Matplotlib, Seaborn, Tableau\\nMachine Learning: Scikit-learn, TensorFlow, Keras\\nBig Data: Hadoop, Spark\\nCertifications\\nCertified Data Scientist, Global Data Certification\\nAdvanced SQL, LinkedIn Learning\\nProjects\\nProject A: Automated data cleaning processes, improving data quality by 40%.\\nProject B: Implemented an NLP solution to analyze customer feedback, enhancing product development.', metadata={'id': 2, 'source': 'Data_Scientist_Resume_3.pdf'})]\n",
      "\n",
      "2 [Document(page_content='Data Scientist Resume\\nContact Information\\nName: Samantha Reed\\nAddress: 456 Oak Lane, Techville, USA\\nPhone: (555) 654-3210\\nEmail: samantha.reed@example.com\\nLinkedIn: linkedin.com/in/samanthareed\\nGitHub: github.com/samanthareed\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, Tech University, 2018\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, Innovatech Ltd., July 2018 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 3, 'source': 'Data_Scientist_Resume_2.pdf'}), Document(page_content='Data Visualization: Matplotlib, Seaborn, Tableau\\nMachine Learning: Scikit-learn, TensorFlow, Keras\\nBig Data: Hadoop, Spark\\nCertifications\\nCertified Data Scientist, Technology Data Institute\\nAdvanced SQL, edX\\nProjects\\nProject A: Led a team in machine learning competitions, ranking in the top 10% globally.\\nProject B: Optimized database queries, reducing data retrieval times by 50%.', metadata={'id': 3, 'source': 'Data_Scientist_Resume_2.pdf'})]\n",
      "\n",
      "2 [Document(page_content='Data Scientist Resume\\nContact Information\\nName: John Doe\\nAddress: 1101 Virtual Road, Computerville, USA\\nPhone: (555) 101-1020\\nEmail: john.doe@example.com\\nLinkedIn: linkedin.com/in/johndoe\\nGitHub: github.com/johndoe\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, Virtual University, 2022\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, DataWiz Solutions, February 2022 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 4, 'source': 'Data_Scientist_Resume_6.pdf'}), Document(page_content='Data Visualization: Matplotlib, Seaborn, Tableau\\nMachine Learning: Scikit-learn, TensorFlow, Keras\\nBig Data: Hadoop, Spark\\nCertifications\\nCertified Data Scientist, Certification Authority for Data Science\\nAdvanced SQL, Pluralsight\\nProjects\\nProject A: Improved data processing speeds by implementing parallel computing strategies.\\nProject B: Created an AI model to predict stock prices with significant accuracy.', metadata={'id': 4, 'source': 'Data_Scientist_Resume_6.pdf'})]\n",
      "\n",
      "2 [Document(page_content='Data Scientist Resume\\nContact Information\\nName: Sophia Martinez\\nAddress: 2025 Innovation Drive, Techburg, USA\\nPhone: (555) 202-2030\\nEmail: sophia.martinez@example.com\\nLinkedIn: linkedin.com/in/sophiamartinez\\nGitHub: github.com/sophiamartinez\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, Techburg University, 2021\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, Innovative Minds Co., March 2021 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 5, 'source': 'Data_Scientist_Resume_7.pdf'}), Document(page_content='Data Visualization: Matplotlib, Seaborn, Tableau\\nMachine Learning: Scikit-learn, TensorFlow, Keras\\nBig Data: Hadoop, Spark\\nCertifications\\nCertified Data Scientist, Advanced Data Science Institute\\nAdvanced SQL, FutureLearn\\nProjects\\nProject A: Led the development of a churn prediction model for a subscription service.\\nProject B: Analyzed large datasets to identify and interpret trends in customer behavior.', metadata={'id': 5, 'source': 'Data_Scientist_Resume_7.pdf'})]\n",
      "\n",
      "2 [Document(page_content='Data Scientist Resume\\nContact Information\\nName: David Lee\\nAddress: 1234 Willow Pass, Datatown, USA\\nPhone: (555) 333-2121\\nEmail: david.lee@example.com\\nLinkedIn: linkedin.com/in/davidlee\\nGitHub: github.com/davidlee\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, University of Big Data, 2021\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, NextGen Data Inc., January 2021 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 6, 'source': 'Data_Scientist_Resume_5.pdf'}), Document(page_content='Data Visualization: Matplotlib, Seaborn, Tableau\\nMachine Learning: Scikit-learn, TensorFlow, Keras\\nBig Data: Hadoop, Spark\\nCertifications\\nCertified Data Scientist, Elite Data Science Institution\\nAdvanced SQL, DataCamp\\nProjects\\nProject A: Conducted a comprehensive market analysis to inform strategic planning.\\nProject B: Collaborated on a cross-functional team to develop a real-time analytics dashboard.', metadata={'id': 6, 'source': 'Data_Scientist_Resume_5.pdf'})]\n",
      "\n",
      "2 [Document(page_content='Data Scientist Resume\\nContact Information\\nName: Emily White\\nAddress: 1010 Maple Street, Silicon Valley, USA\\nPhone: (555) 221-5678\\nEmail: emily.white@example.com\\nLinkedIn: linkedin.com/in/emilywhite\\nGitHub: github.com/emilywhite\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, National University of Sciences, 2017\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, Quantech Analytics, September 2017 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 7, 'source': 'Data_Scientist_Resume_4.pdf'}), Document(page_content='Data Visualization: Matplotlib, Seaborn, Tableau\\nMachine Learning: Scikit-learn, TensorFlow, Keras\\nBig Data: Hadoop, Spark\\nCertifications\\nCertified Data Scientist, Data Science Academy\\nAdvanced SQL, Udemy\\nProjects\\nProject A: Developed predictive models to forecast market trends with a 90% accuracy rate.\\nProject B: Enhanced customer data collection with a web scraping algorithm.', metadata={'id': 7, 'source': 'Data_Scientist_Resume_4.pdf'})]\n",
      "\n",
      "2 [Document(page_content='Data Scientist Resume\\nContact Information\\nName: Olivia Perez\\nAddress: 7890 Ascend St, Summit, USA\\nPhone: (555) 789-0123\\nEmail: olivia.perez@example.com\\nLinkedIn: linkedin.com/in/oliviaperez\\nGitHub: github.com/oliviaperez\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, Summit University, 2018\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, Summit Data Systems, June 2018 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 8, 'source': 'Data_Scientist_Resume_10.pdf'}), Document(page_content='Data Visualization: Matplotlib, Seaborn, Tableau\\nMachine Learning: Scikit-learn, TensorFlow, Keras\\nBig Data: Hadoop, Spark\\nCertifications\\nCertified Data Scientist, Academy of Data Science Excellence\\nAdvanced SQL, Alison\\nProjects\\nProject A: Implemented a system for real-time anomaly detection in network traffic.\\nProject B: Co-authored a paper on machine learning techniques published in a peer-reviewed journal.', metadata={'id': 8, 'source': 'Data_Scientist_Resume_10.pdf'})]\n",
      "\n",
      "2 [Document(page_content='Data Scientist Resume\\nContact Information\\nName: Emma Wilson\\nAddress: 55 New Horizon Blvd, Innovate City, USA\\nPhone: (555) 550-5510\\nEmail: emma.wilson@example.com\\nLinkedIn: linkedin.com/in/emmawilson\\nGitHub: github.com/emmawilson\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, Institute of Advanced Technology, 2019\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, TechPioneers Ltd., May 2019 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 9, 'source': 'Data_Scientist_Resume_9.pdf'}), Document(page_content='Data Visualization: Matplotlib, Seaborn, Tableau\\nMachine Learning: Scikit-learn, TensorFlow, Keras\\nBig Data: Hadoop, Spark\\nCertifications\\nCertified Data Scientist, National Data Science League\\nAdvanced SQL, Codecademy\\nProjects\\nProject A: Pioneered an algorithm for image recognition used in security surveillance.\\nProject B: Developed a chatbot that improved customer service interactions by 40%.', metadata={'id': 9, 'source': 'Data_Scientist_Resume_9.pdf'})]\n",
      "\n",
      "2 [Document(page_content='Data Scientist Resume\\nContact Information\\nName: Lucas Brown\\nAddress: 4033 Discovery Lane, Explore City, USA\\nPhone: (555) 403-4040\\nEmail: lucas.brown@example.com\\nLinkedIn: linkedin.com/in/lucasbrown\\nGitHub: github.com/lucasbrown\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, City College of Exploration, 2020\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, Analytics Engine Co., April 2020 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 10, 'source': 'Data_Scientist_Resume_8.pdf'}), Document(page_content='Data Visualization: Matplotlib, Seaborn, Tableau\\nMachine Learning: Scikit-learn, TensorFlow, Keras\\nBig Data: Hadoop, Spark\\nCertifications\\nCertified Data Scientist, Data Science Global Association\\nAdvanced SQL, Skillshare\\nProjects\\nProject A: Collaborated in a cross-disciplinary team to reduce data processing errors by 30%.\\nProject B: Designed a predictive maintenance system for manufacturing equipment.', metadata={'id': 10, 'source': 'Data_Scientist_Resume_8.pdf'})]\n",
      "\n",
      "2 [Document(page_content='Machine Learning Engineer Resume\\nContact Information\\nName: Charlie Quinn\\nAddress: 8473 Quantum Ave, Sciencetown, USA\\nPhone: (555) 123-9876\\nEmail: charlie.quinn@example.com\\nLinkedIn: linkedin.com/in/charliequinn\\nGitHub: github.com/charliequinn\\nProfessional Summary\\nExperienced Machine Learning Engineer with a strong background in designing, building, and deploying scalable\\nmachine learning models. Adept at data engineering and deploying AI solutions into production environments. Looking\\nto leverage deep learning expertise to tackle new challenges in a dynamic team setting.\\nEducation\\nM.S. in Computer Science, Specialization in Machine Learning, Polytechnic Institute, 2019\\nRelevant Coursework: Deep Learning, Advanced Machine Learning, Distributed Systems\\nWork Experience\\nMachine Learning Engineer, Quantum Computing Insights, January 2019 - Present\\n- Engineered and deployed machine learning pipelines and models for production, resulting in a 25% increase in system\\nefficiency.\\n- Collaborated closely with data scientists and software developers to integrate AI/ML solutions into existing company\\nproducts.\\n- Conducted research and development on state-of-the-art algorithms in computer vision and natural language', metadata={'id': 11, 'source': 'Machine_Learning_Engineer_Resume_3.pdf'}), Document(page_content='processing.\\nSkills\\nProgramming: Python, C++, Java\\nMachine Learning Frameworks: TensorFlow, PyTorch, Keras\\nData Engineering: Apache Kafka, Apache Spark, Hadoop\\nDevOps Tools: Docker, Kubernetes, Jenkins\\nCertifications\\nMachine Learning Certification, Tech Leaders Academy\\nAdvanced Data Engineering with Spark, Skillsoft\\nProjects\\nProject A: Optimized quantum algorithm simulations for materials science research.\\nProject B: Collaborated on developing machine learning models for financial forecasting.', metadata={'id': 11, 'source': 'Machine_Learning_Engineer_Resume_3.pdf'})]\n",
      "\n",
      "2 [Document(page_content='Machine Learning Engineer Resume\\nContact Information\\nName: Avery Walker\\nAddress: 9985 Coding Ln, Devtown, USA\\nPhone: (555) 987-6543\\nEmail: avery.walker@example.com\\nLinkedIn: linkedin.com/in/averywalker\\nGitHub: github.com/averywalker\\nProfessional Summary\\nExperienced Machine Learning Engineer with a strong background in designing, building, and deploying scalable\\nmachine learning models. Adept at data engineering and deploying AI solutions into production environments. Looking\\nto leverage deep learning expertise to tackle new challenges in a dynamic team setting.\\nEducation\\nM.S. in Computer Science, Specialization in Machine Learning, University of Software Engineering, 2021\\nRelevant Coursework: Deep Learning, Advanced Machine Learning, Distributed Systems\\nWork Experience\\nMachine Learning Engineer, Smart Solutions Tech, August 2021 - Present\\n- Engineered and deployed machine learning pipelines and models for production, resulting in a 25% increase in system\\nefficiency.\\n- Collaborated closely with data scientists and software developers to integrate AI/ML solutions into existing company\\nproducts.\\n- Conducted research and development on state-of-the-art algorithms in computer vision and natural language', metadata={'id': 12, 'source': 'Machine_Learning_Engineer_Resume_2.pdf'}), Document(page_content='processing.\\nSkills\\nProgramming: Python, C++, Java\\nMachine Learning Frameworks: TensorFlow, PyTorch, Keras\\nData Engineering: Apache Kafka, Apache Spark, Hadoop\\nDevOps Tools: Docker, Kubernetes, Jenkins\\nCertifications\\nMachine Learning Certification, International AI Society\\nAdvanced Data Engineering with Spark, Udacity\\nProjects\\nProject A: Automated risk assessment models leading to a 20% decrease in operating costs.\\nProject B: Designed and deployed a language translation service using deep learning.', metadata={'id': 12, 'source': 'Machine_Learning_Engineer_Resume_2.pdf'})]\n",
      "\n",
      "2 [Document(page_content='Machine Learning Engineer Resume\\nContact Information\\nName: Riley Morgan\\nAddress: 3301 Tech Park Blvd, Enterprise City, USA\\nPhone: (555) 321-5432\\nEmail: riley.morgan@example.com\\nLinkedIn: linkedin.com/in/rileymorgan\\nGitHub: github.com/rileymorgan\\nProfessional Summary\\nExperienced Machine Learning Engineer with a strong background in designing, building, and deploying scalable\\nmachine learning models. Adept at data engineering and deploying AI solutions into production environments. Looking\\nto leverage deep learning expertise to tackle new challenges in a dynamic team setting.\\nEducation\\nM.S. in Computer Science, Specialization in Machine Learning, College of Engineering and Tech, 2020\\nRelevant Coursework: Deep Learning, Advanced Machine Learning, Distributed Systems\\nWork Experience\\nMachine Learning Engineer, NextGen Robotics, March 2020 - Present\\n- Engineered and deployed machine learning pipelines and models for production, resulting in a 25% increase in system\\nefficiency.\\n- Collaborated closely with data scientists and software developers to integrate AI/ML solutions into existing company\\nproducts.\\n- Conducted research and development on state-of-the-art algorithms in computer vision and natural language', metadata={'id': 13, 'source': 'Machine_Learning_Engineer_Resume_5.pdf'}), Document(page_content='processing.\\nSkills\\nProgramming: Python, C++, Java\\nMachine Learning Frameworks: TensorFlow, PyTorch, Keras\\nData Engineering: Apache Kafka, Apache Spark, Hadoop\\nDevOps Tools: Docker, Kubernetes, Jenkins\\nCertifications\\nMachine Learning Certification, Global Engineering Certification Board\\nAdvanced Data Engineering with Spark, Coursera\\nProjects\\nProject A: Integrated machine learning with robotics to enhance automation in manufacturing.\\nProject B: Led the R&D for an AI-powered drone surveillance system.', metadata={'id': 13, 'source': 'Machine_Learning_Engineer_Resume_5.pdf'})]\n",
      "\n",
      "2 [Document(page_content='Machine Learning Engineer Resume\\nContact Information\\nName: Bailey Jordan\\nAddress: 5466 Innovation Dr, Silicon Area, USA\\nPhone: (555) 654-1122\\nEmail: bailey.jordan@example.com\\nLinkedIn: linkedin.com/in/baileyjordan\\nGitHub: github.com/baileyjordan\\nProfessional Summary\\nExperienced Machine Learning Engineer with a strong background in designing, building, and deploying scalable\\nmachine learning models. Adept at data engineering and deploying AI solutions into production environments. Looking\\nto leverage deep learning expertise to tackle new challenges in a dynamic team setting.\\nEducation\\nM.S. in Computer Science, Specialization in Machine Learning, Technical University of the Valley, 2022\\nRelevant Coursework: Deep Learning, Advanced Machine Learning, Distributed Systems\\nWork Experience\\nMachine Learning Engineer, Cutting Edge AI Labs, February 2022 - Present\\n- Engineered and deployed machine learning pipelines and models for production, resulting in a 25% increase in system\\nefficiency.\\n- Collaborated closely with data scientists and software developers to integrate AI/ML solutions into existing company\\nproducts.\\n- Conducted research and development on state-of-the-art algorithms in computer vision and natural language', metadata={'id': 14, 'source': 'Machine_Learning_Engineer_Resume_4.pdf'}), Document(page_content='processing.\\nSkills\\nProgramming: Python, C++, Java\\nMachine Learning Frameworks: TensorFlow, PyTorch, Keras\\nData Engineering: Apache Kafka, Apache Spark, Hadoop\\nDevOps Tools: Docker, Kubernetes, Jenkins\\nCertifications\\nMachine Learning Certification, AI & Machine Learning Association\\nAdvanced Data Engineering with Spark, edX\\nProjects\\nProject A: Engineered an advanced recommendation system for streaming services.\\nProject B: Developed a predictive model for patient diagnosis in telemedicine.', metadata={'id': 14, 'source': 'Machine_Learning_Engineer_Resume_4.pdf'})]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "documents = []\n",
    "\n",
    "for idx, file in enumerate(filenames):\n",
    "    loader = PyPDFLoader(data_root + file)\n",
    "    document = loader.load()\n",
    "    for document_fragment in document:\n",
    "        document_fragment.metadata = metadata[idx]\n",
    "        \n",
    "    print(f'{len(document)} {document}\\n')\n",
    "    documents += document\n",
    "\n",
    "\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "# Before we are proceeding we are looking into some interesting statistics regarding the document preprocessing we just performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length among 30 documents loaded is 778 characters.\n",
      "After the split we have 45 documents as opposed to the original 30.\n",
      "Average length among 45 documents (after split) is 519 characters.\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "print(f'Average length among {len(documents)} documents loaded is {avg_doc_length(documents)} characters.')\n",
    "print(f'After the split we have {len(docs)} documents as opposed to the original {len(documents)}.')\n",
    "print(f'Average length among {len(docs)} documents (after split) is {avg_doc_length(docs)} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had 3 PDF documents and one txt file which have been split into smaller ~500 chunks.\n",
    "\n",
    "Now we can see how a sample embedding would look like for one of those chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embedding of a document chunk:  [ 0.58984375  0.12207031 -0.00946045 ...  0.4296875  -0.48242188\n",
      "  0.15625   ]\n",
      "Size of the embedding:  (1536,)\n"
     ]
    }
   ],
   "source": [
    "sample_embedding = np.array(bedrock_embeddings.embed_query(docs[0].page_content))\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the very same approach embeddings can be generated for the entire corpus and stored in a vector store.\n",
    "\n",
    "This can be easily done using [FAISS](https://github.com/facebookresearch/faiss) implementation inside [LangChain](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html) which takes  input the embeddings model and the documents to create the entire vector store. Using the Index Wrapper we can abstract away most of the heavy lifting such as creating the prompt, getting embeddings of the query, sampling the relevant documents and calling the LLM. [VectorStoreIndexWrapper](https://python.langchain.com/en/latest/modules/indexes/getting_started.html#one-line-index-creation) helps us with that.\n",
    "\n",
    "**⚠️⚠️⚠️ NOTE: it might take few minutes to run the following cell ⚠️⚠️⚠️**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    ")\n",
    "\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# test on using loaded documents directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    documents, #load documents from pdfs\n",
    "    bedrock_embeddings,\n",
    ")\n",
    "\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents are fetched which are relevant to the query.\n",
      "----\n",
      "## Document 1: Data Scientist Resume\n",
      "Contact Information\n",
      "Name: Samantha Reed\n",
      "Address: 456 Oak Lane, Techville, USA\n",
      "Phone: (555) 654-3210\n",
      "Email: samantha.reed@example.com\n",
      "LinkedIn: linkedin.com/in/samanthareed\n",
      "GitHub: github.com/samanthareed\n",
      "Professional Summary\n",
      "Highly skilled and analytical Data Scientist with experience in machine learning, data mining, and\n",
      "data visualization.\n",
      "Proven ability to drive business decisions by providing actionable insights from data analytics.\n",
      "Proficient in statistical\n",
      "software and programming languages including Python, R, and SQL.\n",
      "Education\n",
      "B.Sc. in Data Science, Tech University, 2018\n",
      "Relevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\n",
      "Work Experience\n",
      "Data Scientist, Innovatech Ltd., July 2018 - Present\n",
      "- Developed and implemented statistical models for data analysis that increased sales by 15%.\n",
      "- Created data visualizations to communicate complex analysis and insights to non-technical team\n",
      "members.\n",
      "- Managed the data science project lifecycle from data collection to deployment.\n",
      "Skills\n",
      "Programming: Python, R, SQL, Java.......\n",
      "---\n",
      "## Document 2: Data Scientist Resume\n",
      "Contact Information\n",
      "Name: David Lee\n",
      "Address: 1234 Willow Pass, Datatown, USA\n",
      "Phone: (555) 333-2121\n",
      "Email: david.lee@example.com\n",
      "LinkedIn: linkedin.com/in/davidlee\n",
      "GitHub: github.com/davidlee\n",
      "Professional Summary\n",
      "Highly skilled and analytical Data Scientist with experience in machine learning, data mining, and\n",
      "data visualization.\n",
      "Proven ability to drive business decisions by providing actionable insights from data analytics.\n",
      "Proficient in statistical\n",
      "software and programming languages including Python, R, and SQL.\n",
      "Education\n",
      "B.Sc. in Data Science, University of Big Data, 2021\n",
      "Relevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\n",
      "Work Experience\n",
      "Data Scientist, NextGen Data Inc., January 2021 - Present\n",
      "- Developed and implemented statistical models for data analysis that increased sales by 15%.\n",
      "- Created data visualizations to communicate complex analysis and insights to non-technical team\n",
      "members.\n",
      "- Managed the data science project lifecycle from data collection to deployment.\n",
      "Skills\n",
      "Programming: Python, R, SQL, Java.......\n",
      "---\n",
      "## Document 3: Data Scientist Resume\n",
      "Contact Information\n",
      "Name: Sophia Martinez\n",
      "Address: 2025 Innovation Drive, Techburg, USA\n",
      "Phone: (555) 202-2030\n",
      "Email: sophia.martinez@example.com\n",
      "LinkedIn: linkedin.com/in/sophiamartinez\n",
      "GitHub: github.com/sophiamartinez\n",
      "Professional Summary\n",
      "Highly skilled and analytical Data Scientist with experience in machine learning, data mining, and\n",
      "data visualization.\n",
      "Proven ability to drive business decisions by providing actionable insights from data analytics.\n",
      "Proficient in statistical\n",
      "software and programming languages including Python, R, and SQL.\n",
      "Education\n",
      "B.Sc. in Data Science, Techburg University, 2021\n",
      "Relevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\n",
      "Work Experience\n",
      "Data Scientist, Innovative Minds Co., March 2021 - Present\n",
      "- Developed and implemented statistical models for data analysis that increased sales by 15%.\n",
      "- Created data visualizations to communicate complex analysis and insights to non-technical team\n",
      "members.\n",
      "- Managed the data science project lifecycle from data collection to deployment.\n",
      "Skills\n",
      "Programming: Python, R, SQL, Java.......\n",
      "---\n",
      "## Document 4: Data Scientist Resume\n",
      "Contact Information\n",
      "Name: Alex Johnson\n",
      "Address: 123 Main St, Anytown, USA\n",
      "Phone: (555) 123-4567\n",
      "Email: alex.johnson@example.com\n",
      "LinkedIn: linkedin.com/in/alexjohnson\n",
      "GitHub: github.com/alexjohnson\n",
      "Professional Summary\n",
      "Highly skilled and analytical Data Scientist with experience in machine learning, data mining, and\n",
      "data visualization.\n",
      "Proven ability to drive business decisions by providing actionable insights from data analytics.\n",
      "Proficient in statistical\n",
      "software and programming languages including Python, R, and SQL.\n",
      "Education\n",
      "B.Sc. in Data Science, State University, 2019\n",
      "Relevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\n",
      "Work Experience\n",
      "Data Scientist, Tech Solutions Inc., June 2019 - Present\n",
      "- Developed and implemented statistical models for data analysis that increased sales by 15%.\n",
      "- Created data visualizations to communicate complex analysis and insights to non-technical team\n",
      "members.\n",
      "- Managed the data science project lifecycle from data collection to deployment.\n",
      "Skills\n",
      "Programming: Python, R, SQL, Java.......\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "query = \"find good candidate for data scientist position?\"\n",
    "query_embedding = vectorstore_faiss.embedding_function(query)\n",
    "\n",
    "relevant_documents = vectorstore_faiss.similarity_search_by_vector(query_embedding)\n",
    "print(f'{len(relevant_documents)} documents are fetched which are relevant to the query.')\n",
    "print('----')\n",
    "for i, rel_doc in enumerate(relevant_documents):\n",
    "    print_ww(f'## Document {i+1}: {rel_doc.page_content}.......')\n",
    "    print('---')\n",
    "### Question Answering\n",
    "\n",
    "# Now that we have our vector store in place, we can start asking questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the relevant documents, it's time to use the LLM to generate an answer based on these documents. \n",
    "\n",
    "We will take our inital prompt, together with our relevant documents which were retreived based on the results of our similarity search. We then by combining these create a prompt that we feed back to the model to get our result. At this point our model should give us highly informed information on how we can change the tire of our specific car as it was outlined in our manual.\n",
    "\n",
    "LangChain provides an abstraction of how this can be done easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick way\n",
    "You have the possibility to use the wrapper provided by LangChain which wraps around the Vector Store and takes input the LLM.\n",
    "This wrapper performs the following steps behind the scences:\n",
    "- Takes input the question\n",
    "- Create question embedding\n",
    "- Fetch relevant documents\n",
    "- Stuff the documents and the question into a prompt\n",
    "- Invoke the model with the prompt and generate the answer in a human readable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided resumes, here are the top 2 candidates for the data scientist position:\n",
      "\n",
      "1. Samantha Reed\n",
      "- B.Sc. in Data Science from Tech University in 2018, so has 3 years of experience\n",
      "- Currently working as Data Scientist at Innovatech Ltd since 2018\n",
      "- Developed statistical models that increased sales by 15%\n",
      "- Strong programming skills in Python, R, SQL, Java\n",
      "\n",
      "2. Alex Johnson\n",
      "- B.Sc. in Data Science from State University in 2019, so has 2 years of experience\n",
      "- Currently working as Data Scientist at Tech Solutions Inc since 2019\n",
      "- Also developed statistical models that increased sales by 15%\n",
      "- Strong programming skills in Python, R, SQL, Java\n",
      "\n",
      "David Lee seems to be a less strong candidate based on having only 1 year of experience since\n",
      "graduating in 2021. But Samantha Reed and Alex Johnson both have impressive resumes showing\n",
      "successful data science projects and technical\n"
     ]
    }
   ],
   "source": [
    "query = \"find top candidates for data scientist position?\"\n",
    "\n",
    "answer = wrapper_store_faiss.query(question=query, llm=llm)\n",
    "print_ww(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask a different question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the resumes provided, here are some potential good machine learning engineer candidates:\n",
      "\n",
      "- Avery Walker - Has an MS in Computer Science with a specialization in machine learning. Worked as\n",
      "a ML engineer at Smart Solutions Tech, where he deployed ML models and pipelines that increased\n",
      "efficiency by 25%. Has experience with deep learning and computer vision.\n",
      "\n",
      "- Riley Morgan - Also has an advanced degree in CS with ML specialization. Worked as a ML engineer\n",
      "at NextGen Robotics deploying models and collaborating with data scientists. Has R&D experience in\n",
      "CV and NLP.\n",
      "\n",
      "- Bailey Jordan - Recently completed an MS in CS with an ML focus. Worked at Cutting Edge AI Labs\n",
      "deploying ML models and pipelines. Has experience with deep learning and advanced ML algorithms.\n",
      "\n",
      "All three have the educational background, work experience, and technical skills that would make\n",
      "them strong candidates for ML engineering roles. Their resumes highlight hands-on experience\n",
      "building, deploy\n"
     ]
    }
   ],
   "source": [
    "query_2 = \"find some good machine learning engineers?\"\n",
    "\n",
    "answer_2 = wrapper_store_faiss.query(question=query_2, llm=llm)\n",
    "print_ww(answer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the information provided, there are no mentions of any of the machine learning engineers\n",
      "having soccer skills. The context focuses on their programming skills, machine learning expertise,\n",
      "data engineering knowledge, projects and certifications, but does not indicate if any of them have\n",
      "experience or skills related to soccer specifically. Since no soccer skills are listed, I don't have\n",
      "enough information to determine which machine learning engineer has soccer skills.\n"
     ]
    }
   ],
   "source": [
    "query_3 = \"which machine learning engineer has soccer skills\"\n",
    "\n",
    "answer_3 = wrapper_store_faiss.query(question=query_3, llm=llm)\n",
    "print_ww(answer_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customisable option\n",
    "In the above scenario you explored the quick and easy way to get a context-aware answer to your question. Now let's have a look at a more customizable option with the help of [RetrievalQA](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html) where you can customize how the documents fetched should be added to prompt using `chain_type` parameter. Also, if you want to control how many relevant documents should be retrieved then change the `k` parameter in the cell below to see different outputs. In many scenarios you might want to know which were the source documents that the LLM used to generate the answer, you can get those documents in the output using `return_source_documents` which returns the documents that are added to the context of the LLM prompt. `RetrievalQA` also allows you to provide a custom [prompt template](https://python.langchain.com/en/latest/modules/prompts/prompt_templates/getting_started.html) which can be specific to the model.\n",
    "\n",
    "Note: In this example we are using Anthropic Claude as the LLM under Amazon Bedrock, this particular model performs best if the inputs are provided under `Human:` and the model is requested to generate an output after `Assistant:`. In the cell below you see an example of how to control the prompt such that the LLM stays grounded and doesn't answer outside the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "\n",
    "Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    callbacks=[token_counter]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token Counts:\n",
      "Total: 12571\n",
      "Embedding: N/A\n",
      "Prompt: 10615\n",
      "Generation:1956\n",
      "\n",
      " Based on the resumes provided, Alex Johnson seems to be a strong candidate for a data scientist\n",
      "role. The resume highlights relevant education in data science, coursework in machine learning and\n",
      "statistics, and professional experience as a data scientist. The skills listed also align well with\n",
      "common data scientist requirements. Without more information I cannot make a more definitive\n",
      "recommendation, but Alex Johnson's resume indicates he would likely be a good fit for a data\n",
      "scientist position.\n",
      "\n",
      "[Document(page_content='Data Scientist Resume\\nContact Information\\nName: David Lee\\nAddress: 1234 Willow Pass, Datatown, USA\\nPhone: (555) 333-2121\\nEmail: david.lee@example.com\\nLinkedIn: linkedin.com/in/davidlee\\nGitHub: github.com/davidlee\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, University of Big Data, 2021\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, NextGen Data Inc., January 2021 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 6, 'source': 'Data_Scientist_Resume_5.pdf'}), Document(page_content='Data Scientist Resume\\nContact Information\\nName: Alex Johnson\\nAddress: 123 Main St, Anytown, USA\\nPhone: (555) 123-4567\\nEmail: alex.johnson@example.com\\nLinkedIn: linkedin.com/in/alexjohnson\\nGitHub: github.com/alexjohnson\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, State University, 2019\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, Tech Solutions Inc., June 2019 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 0, 'source': 'Data_Scientist_Resume_0.pdf'}), Document(page_content='Data Scientist Resume\\nContact Information\\nName: Alex Johnson\\nAddress: 123 Main St, Anytown, USA\\nPhone: (555) 123-4567\\nEmail: alex.johnson@example.com\\nLinkedIn: linkedin.com/in/alexjohnson\\nGitHub: github.com/alexjohnson\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, State University, 2019\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, Tech Solutions Inc., June 2019 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 1, 'source': 'Data_Scientist_Resume_1.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"recommend good data scientist candidates?\"\n",
    "result = qa({\"query\": query})\n",
    "print_ww(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "\n",
    "Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Try to match candidate with following criteria\n",
    "Professional Summary\n",
    "Experienced Machine Learning Engineer with a strong background in designing, building, and deploying scalable\n",
    "machine learning models. Adept at data engineering and deploying AI solutions into production environments. Looking\n",
    "to leverage deep learning expertise to tackle new challenges in a dynamic team setting.\n",
    "Education\n",
    "M.S. in Computer Science, Specialization in Machine Learning\n",
    "Relevant Coursework: Deep Learning, Advanced Machine Learning, Distributed Systems\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    callbacks=[token_counter]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token Counts:\n",
      "Total: 13452\n",
      "Embedding: N/A\n",
      "Prompt: 11345\n",
      "Generation:2107\n",
      "\n",
      " Based on the provided information, Riley Morgan seems to match the criteria for a machine learning\n",
      "engineer well. The key factors are:\n",
      "\n",
      "- Relevant education (M.S. in Computer Science with specialization in ML)\n",
      "\n",
      "- Work experience as a machine learning engineer at NextGen Robotics, including deploying ML models\n",
      "to production and collaborating with data scientists.\n",
      "\n",
      "- Specific ML projects mentioned, like increasing system efficiency by 25% with ML pipelines.\n",
      "\n",
      "- Familiarity with relevant ML tools and techniques like TensorFlow, Keras, computer vision, and\n",
      "NLP.\n",
      "\n",
      "The other candidate Avery Walker also has strong ML credentials, but Riley Morgan's work experience\n",
      "seems like the closest match for the role based on the criteria provided. Riley would be a good data\n",
      "machine learning engineer candidate to recommend.\n",
      "\n",
      "[Document(page_content='Data Visualization: Matplotlib, Seaborn, Tableau\\nMachine Learning: Scikit-learn, TensorFlow, Keras\\nBig Data: Hadoop, Spark\\nCertifications\\nCertified Data Scientist, Technology Data Institute\\nAdvanced SQL, edX\\nProjects\\nProject A: Led a team in machine learning competitions, ranking in the top 10% globally.\\nProject B: Optimized database queries, reducing data retrieval times by 50%.', metadata={'id': 3, 'source': 'Data_Scientist_Resume_2.pdf'}), Document(page_content='Machine Learning Engineer Resume\\nContact Information\\nName: Avery Walker\\nAddress: 9985 Coding Ln, Devtown, USA\\nPhone: (555) 987-6543\\nEmail: avery.walker@example.com\\nLinkedIn: linkedin.com/in/averywalker\\nGitHub: github.com/averywalker\\nProfessional Summary\\nExperienced Machine Learning Engineer with a strong background in designing, building, and deploying scalable\\nmachine learning models. Adept at data engineering and deploying AI solutions into production environments. Looking\\nto leverage deep learning expertise to tackle new challenges in a dynamic team setting.\\nEducation\\nM.S. in Computer Science, Specialization in Machine Learning, University of Software Engineering, 2021\\nRelevant Coursework: Deep Learning, Advanced Machine Learning, Distributed Systems\\nWork Experience\\nMachine Learning Engineer, Smart Solutions Tech, August 2021 - Present\\n- Engineered and deployed machine learning pipelines and models for production, resulting in a 25% increase in system\\nefficiency.\\n- Collaborated closely with data scientists and software developers to integrate AI/ML solutions into existing company\\nproducts.\\n- Conducted research and development on state-of-the-art algorithms in computer vision and natural language', metadata={'id': 12, 'source': 'Machine_Learning_Engineer_Resume_2.pdf'}), Document(page_content='Machine Learning Engineer Resume\\nContact Information\\nName: Riley Morgan\\nAddress: 3301 Tech Park Blvd, Enterprise City, USA\\nPhone: (555) 321-5432\\nEmail: riley.morgan@example.com\\nLinkedIn: linkedin.com/in/rileymorgan\\nGitHub: github.com/rileymorgan\\nProfessional Summary\\nExperienced Machine Learning Engineer with a strong background in designing, building, and deploying scalable\\nmachine learning models. Adept at data engineering and deploying AI solutions into production environments. Looking\\nto leverage deep learning expertise to tackle new challenges in a dynamic team setting.\\nEducation\\nM.S. in Computer Science, Specialization in Machine Learning, College of Engineering and Tech, 2020\\nRelevant Coursework: Deep Learning, Advanced Machine Learning, Distributed Systems\\nWork Experience\\nMachine Learning Engineer, NextGen Robotics, March 2020 - Present\\n- Engineered and deployed machine learning pipelines and models for production, resulting in a 25% increase in system\\nefficiency.\\n- Collaborated closely with data scientists and software developers to integrate AI/ML solutions into existing company\\nproducts.\\n- Conducted research and development on state-of-the-art algorithms in computer vision and natural language', metadata={'id': 13, 'source': 'Machine_Learning_Engineer_Resume_5.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"recommend good data machine learning engineer?\"\n",
    "result = qa({\"query\": query})\n",
    "print_ww(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token Counts:\n",
      "Total: 3861\n",
      "Embedding: N/A\n",
      "Prompt: 3327\n",
      "Generation:534\n",
      "\n",
      " Based on the resumes provided, Alex Johnson seems to be a good candidate for a data scientist role.\n",
      "Alex has a B.Sc. in Data Science, relevant coursework and work experience as a Data Scientist at\n",
      "Tech Solutions Inc. since 2019. The resume highlights Alex's skills in developing and implementing\n",
      "statistical models, data analysis, visualization, and managing data science projects.\n",
      "\n",
      "[Document(page_content='Data Scientist Resume\\nContact Information\\nName: David Lee\\nAddress: 1234 Willow Pass, Datatown, USA\\nPhone: (555) 333-2121\\nEmail: david.lee@example.com\\nLinkedIn: linkedin.com/in/davidlee\\nGitHub: github.com/davidlee\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, University of Big Data, 2021\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, NextGen Data Inc., January 2021 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 6, 'source': 'Data_Scientist_Resume_5.pdf'}), Document(page_content='Data Scientist Resume\\nContact Information\\nName: Alex Johnson\\nAddress: 123 Main St, Anytown, USA\\nPhone: (555) 123-4567\\nEmail: alex.johnson@example.com\\nLinkedIn: linkedin.com/in/alexjohnson\\nGitHub: github.com/alexjohnson\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, State University, 2019\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, Tech Solutions Inc., June 2019 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 0, 'source': 'Data_Scientist_Resume_0.pdf'}), Document(page_content='Data Scientist Resume\\nContact Information\\nName: Alex Johnson\\nAddress: 123 Main St, Anytown, USA\\nPhone: (555) 123-4567\\nEmail: alex.johnson@example.com\\nLinkedIn: linkedin.com/in/alexjohnson\\nGitHub: github.com/alexjohnson\\nProfessional Summary\\nHighly skilled and analytical Data Scientist with experience in machine learning, data mining, and data visualization.\\nProven ability to drive business decisions by providing actionable insights from data analytics. Proficient in statistical\\nsoftware and programming languages including Python, R, and SQL.\\nEducation\\nB.Sc. in Data Science, State University, 2019\\nRelevant Coursework: Machine Learning, Statistical Analysis, Big Data Analytics\\nWork Experience\\nData Scientist, Tech Solutions Inc., June 2019 - Present\\n- Developed and implemented statistical models for data analysis that increased sales by 15%.\\n- Created data visualizations to communicate complex analysis and insights to non-technical team members.\\n- Managed the data science project lifecycle from data collection to deployment.\\nSkills\\nProgramming: Python, R, SQL, Java', metadata={'id': 1, 'source': 'Data_Scientist_Resume_1.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"recommend good data scientist candidates?\"\n",
    "result = qa({\"query\": query})\n",
    "print_ww(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/htang/Documents/amazon-bedrock-rag-workshop/05_test/01_RAG_with_Semantic_Search_Titan_Embeddings_Claude_Modified.ipynb Cell 37\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/htang/Documents/amazon-bedrock-rag-workshop/05_test/01_RAG_with_Semantic_Search_Titan_Embeddings_Claude_Modified.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHow was Amazon impacted by COVID-19?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/htang/Documents/amazon-bedrock-rag-workshop/05_test/01_RAG_with_Semantic_Search_Titan_Embeddings_Claude_Modified.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m result \u001b[39m=\u001b[39m qa({\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m: query})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/htang/Documents/amazon-bedrock-rag-workshop/05_test/01_RAG_with_Semantic_Search_Titan_Embeddings_Claude_Modified.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m print_ww(result[\u001b[39m'\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/htang/Documents/amazon-bedrock-rag-workshop/05_test/01_RAG_with_Semantic_Search_Titan_Embeddings_Claude_Modified.ipynb#X52sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mresult[\u001b[39m'\u001b[39m\u001b[39msource_documents\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qa' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"How was Amazon impacted by COVID-19?\"\n",
    "\n",
    "result = qa({\"query\": query})\n",
    "\n",
    "print_ww(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this moduel on retrieval augmented generation! This is an important technique that combines the power of large language models with the precision of retrieval methods. By augmenting generation with relevant retrieved examples, the responses we recieved become more coherent, consistent and grounded. You should feel proud of learning this innovative approach. I'm sure the knowledge you've gained will be very useful for building creative and engaging language generation systems. Well done!\n",
    "\n",
    "In the above implementation of RAG based Question Answering we have explored the following concepts and how to implement them using Amazon Bedrock and it's LangChain integration.\n",
    "\n",
    "- Loading documents of different kind and generating embeddings to create a vector store\n",
    "- Retrieving documents to the question\n",
    "- Preparing a prompt which goes as input to the LLM\n",
    "- Present an answer in a human friendly manner\n",
    "\n",
    "### Take-aways\n",
    "- Experiment with different Vector Stores\n",
    "- Leverage various models available under Amazon Bedrock to see alternate outputs\n",
    "- Explore options such as persistent storage of embeddings and document chunks\n",
    "- Integration with enterprise data stores\n",
    "\n",
    "# Thank You"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "bedragenv",
   "language": "python",
   "name": "bedragenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
